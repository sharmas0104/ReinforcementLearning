{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Dependencies\n",
    "\n",
    "Stable baselines is an RL library that allows you to work with model free algorithms. Runs on Tensorflow and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/46/18/cdee520cbd3761e202163f035b647ab1e12b57fd9b7b8c594673a01e4448/tensorflow-2.6.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting gast==0.4.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
      "Collecting google-pasta~=0.2 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Collecting termcolor~=1.1.0 (from tensorflow)\n",
      "Collecting flatbuffers~=1.12.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
      "Collecting tensorflow-estimator~=2.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/54/1b2f1e22a2670546cc02e4df1b80425edaee02133173bb91aa0f6d3d0293/tensorflow_estimator-2.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Collecting h5py~=3.1.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/53/c2/77bd81922264520b492bd7bfd1a51a845bc1187445408a7a83db284fd566/h5py-3.1.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting typing-extensions~=3.7.4 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting keras~=2.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/5a/38/04d9b7fb53acdf861df2c4505fa96b06c779817a511e94b8882d284ba360/keras-2.6.0-py2.py3-none-any.whl\n",
      "Collecting clang~=5.0 (from tensorflow)\n",
      "Collecting wheel~=0.35 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/04/80/cad93b40262f5d09f6de82adbee452fd43cdff60830b56a74c5930f7e277/wheel-0.37.0-py2.py3-none-any.whl\n",
      "Collecting wrapt~=1.12.1 (from tensorflow)\n",
      "Collecting opt-einsum~=3.3.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl\n",
      "Collecting six~=1.15.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting astunparse~=1.6.3 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\sharma1507\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow) (1.19.5)\n",
      "Collecting keras-preprocessing~=1.1.2 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (41.4.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Collecting cached-property; python_version < \"3.8\" (from h5py~=3.1.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (0.23)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (7.2.0)\n",
      "Installing collected packages: gast, six, google-pasta, termcolor, flatbuffers, tensorflow-estimator, cached-property, h5py, typing-extensions, keras, clang, wheel, wrapt, opt-einsum, astunparse, keras-preprocessing, tensorflow\n",
      "Successfully installed astunparse-1.6.3 cached-property-1.5.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-pasta-0.2.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 opt-einsum-3.3.0 six-1.15.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wheel-0.37.0 wrapt-1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: astroid 2.3.1 requires typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\", which is not installed.\n",
      "ERROR: astroid 2.3.1 has requirement six==1.12, but you'll have six 1.15.0 which is incompatible.\n",
      "ERROR: astroid 2.3.1 has requirement wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\n",
      "  WARNING: The script wheel.exe is installed in 'C:\\Users\\sharma1507\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\sharma1507\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sharma1507\\appdata\\roaming\\python\\python37\\site-packages (from stable-baselines3[extra]) (1.21.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.25.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.9.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: gym>=0.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.19.0)\n",
      "Requirement already satisfied: opencv-python; extra == \"extra\" in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.3.0.36)\n",
      "Requirement already satisfied: tensorboard>=2.2.0; extra == \"extra\" in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.6.0)\n",
      "Requirement already satisfied: pillow; extra == \"extra\" in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (6.2.0)\n",
      "Requirement already satisfied: psutil; extra == \"extra\" in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.6.3)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in c:\\programdata\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.2.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2019.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.10.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.5)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.39.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.35.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.33.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.17.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (41.4.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.13.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from atari-py~=0.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.24.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.23)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # operating system library that makes it easier to define our paths to save model as well as where to log out\n",
    "import gym #for OpenAI gym, allows us to build environments and work with pre-existing environments\n",
    "from stable_baselines3 import PPO # an algorithm\n",
    " #stable baselines allows you to vectorize your environments, making it more practical to train your agent on multiple environments at the same time. Boosts training speed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv #not really vectorization, more like a wrapper around env that makes it easier to work with stable baselines\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #  test out how a model is performing. Gets the average reward over a certain number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyglet\n",
      "  Using cached https://files.pythonhosted.org/packages/48/c2/5898d5cce5d5ce7e74b5a515f2d107a82f2c4d0d4505c0ca119cb34c6b01/pyglet-1.5.19-py3-none-any.whl\n",
      "Installing collected packages: pyglet\n",
      "Successfully installed pyglet-1.5.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload environment\n",
    "environment_name = 'CartPole-v0'\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-585225a40d86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# visual representation of env.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# generate a random action, NOT an action informed by observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mn_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# pass through random action -a forward pass, or in this case,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m#supply an action to the environment, gets an observation back.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# get back the next set of observations, the reward for taking the inputted actio)n (positive for increase, negative for decrease (includes 0).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \"\"\"\n\u001b[0;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             )\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# test out environments, seeing how the agent can interact with it. \n",
    "episodes = 5 # test 5 times\n",
    "for episode in range(1, episodes + 1): # loop through each episode\n",
    "    state = env.reset()  # reset environment every time theres a new episode, get an initial set of observations\n",
    "    # these observations are passed to the reinforcement agent to determine best action to maximize reward, but we aren't doing that right now\n",
    "    done = False # episode is not done\n",
    "    score = 0 \n",
    "    \n",
    "    #actions will move bar to the left and to the right\n",
    "    while not done: \n",
    "        env.render() # visual representation of env. \n",
    "        action = env.action_space.sample() # generate a random action, NOT an action informed by observations\n",
    "        n_state, reward, done, info = env.step(action) # pass through random action -a forward pass, or in this case, \n",
    "        #supply an action to the environment, gets an observation back.\n",
    "        # get back the next set of observations, the reward for taking the inputted actio)n (positive for increase, negative for decrease (includes 0). \n",
    "        #whether episode is done. If done, stop. )\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "#env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02500592,  0.0207521 , -0.04510446, -0.01304501])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # observations for observations space. can take these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # get two different types of actions, either 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different spaces within any environment: the action space and the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.9326792e+00,  1.3847042e+38,  3.3825469e-01, -2.5008390e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample() # also randomly outputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, there's two parts of our environment, actions space and observation space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do the action space values mean?\n",
    "\n",
    "Type = Discrete(2)\n",
    "0: Push cart to the left, 1: push cart to the right\n",
    "\n",
    "Reward is 1 for every step taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do these observation spaces values mean?\n",
    "\n",
    "You can take a look at the openAI documentation to find out, but here they are: \n",
    "\n",
    "Type: Box(4)\n",
    "\n",
    "Num 0: Cart Position , from [-4.8 , 4/8]\n",
    "\n",
    "Num 1: Cart Velicity, (-Inf, Inf)\n",
    "\n",
    "Num 2: Pole Angle, [-24 degrees, 24 degrees]\n",
    "\n",
    "Num 3: Pole Angular Velocity, all reals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.5754972e+00,  2.6312240e+38,  2.7803665e-01,  1.0333603e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our agent for real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location to save our tensor board logs: good for monitoring and referencing about how model is performing\n",
    "log_path = os.path.join('Training', 'Logs') # so, inside the folder that you're working in, creating a folder called Training. The Training folder has another folder inside of it called Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# instantiate algorithm. Using PPO\n",
    "env = gym.make(environment_name) \n",
    "env = DummyVecEnv([lambda: env]) #wrap environment\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path) # define model, \n",
    "#MlpPolicy - using a neural network\n",
    "# verbose = 1: meaning please log out results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You need to look at the documentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_7\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 386  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008132798 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.000343    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.67        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 52.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 723         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008817364 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.0903      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009203671 |\n",
      "|    clip_fraction        | 0.0857      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 53.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007954333 |\n",
      "|    clip_fraction        | 0.0612      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.622      |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 63.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 934         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012060961 |\n",
      "|    clip_fraction        | 0.0967      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.1        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 73.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 976         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004892088 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 78.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1010         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049364506 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.572        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00704     |\n",
      "|    value_loss           | 66.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1038         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021549647 |\n",
      "|    clip_fraction        | 0.00801      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | 0.429        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.8         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 78.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1063        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006431859 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    value_loss           | 62          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x287480acf48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_Model_Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2467 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x287480acf48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after deleting, you can actually \"recover it\" by reloading. Reload it back into memory\n",
    "model = PPO.load(PPO_Path, env= env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "For PPO, an evinroment is considered solved if you get on avergae a reward of 200 or higher\n",
    "Test our model to see how it's performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the documentation for this, too\n",
    "evaluate_policy(model, env, n_eval_episodes = 10, render = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the output: The first coordinate is the average rewards and the second is the STDev\n",
    "\n",
    "\n",
    "Reward for CartPole is calculated as 1 point for every step that the pole reamins upright (with a max of 200 steps). If the pole is more than 15 degrees from vertical or the cart moves more than 2.4 units from the center the episode ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[200.]\n",
      "Episode:2 Score:[200.]\n",
      "Episode:3 Score:[200.]\n",
      "Episode:4 Score:[200.]\n",
      "Episode:5 Score:[200.]\n"
     ]
    }
   ],
   "source": [
    "# test out environments -- uhh what is the difference from what we did before\n",
    "episodes = 5 # test 5 times\n",
    "for episode in range(1, episodes + 1): # loop through each episode\n",
    "    obs = env.reset()  # reset environment every time theres a new episode, get an initial set of observations\n",
    "    # these observations are passed to the reinforcement agent to determine best action to maximize reward\n",
    "    done = False # episode is not done\n",
    "    score = 0 \n",
    "    \n",
    "    #actions will move bar to the left and to the right\n",
    "    while not done: \n",
    "        env.render() # visual representation of env. \n",
    "        action, _ = model.predict(obs) # NOW USING MODEL HERE\n",
    "        obs, reward, done, info = env.step(action) # pass through random action -a forward pass, or in this case, \n",
    "        #supply an action to the environment, gets an observation back.\n",
    "        # get back the next set of observations, the reward for taking the inputted actio)n (positive for increase, negative for decrease (includes 0). \n",
    "        #whether episode is done. If done, stop. )\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Logs in Tensorboard\n",
    "\n",
    "It's a good idea to use this when you have a much larger or more complex environment. Ideally, it's best to use this in a command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs\\\\PPO_8'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get log directory that you want to view\n",
    "# specify the training log path\n",
    "training_log_path = os.path.join(log_path, 'PPO_8')\n",
    "\n",
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-01 12:24:07.998972: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-09-01 12:24:07.999165: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
      "                   [--host ADDR] [--bind_all] [--port PORT]\n",
      "                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
      "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
      "                   [--grpc_creds_type {local,ssl,ssl_dev}]\n",
      "                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n",
      "                   [--db URI] [--db_import] [--inspect] [--version_tb]\n",
      "                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
      "                   [--window_title TEXT] [--max_reload_threads COUNT]\n",
      "                   [--reload_interval SECONDS] [--reload_task TYPE]\n",
      "                   [--reload_multifile BOOL]\n",
      "                   [--reload_multifile_inactive_secs SECONDS]\n",
      "                   [--generic_data TYPE]\n",
      "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
      "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
      "                   [--whatif-data-dir PATH]\n",
      "                   {serve,dev} ...\n",
      "tensorboard: error: invalid choice: 'Training\\\\Logs\\\\PPO_2' (choose from 'serve', 'dev')\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir = {training_log_path}\n",
    "\n",
    "# break down this line\n",
    "# using an exclamation mark inside a jupyter notebook is known as a \"magic command\",allows you to run command line\n",
    "# prompts in your notebook\n",
    "\n",
    "\n",
    "# see if this works on your mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Core metrics to look at when you are testing your model performance are your average reward, average episode length. \n",
    "\n",
    "Average Episode Length: \n",
    "\n",
    "-see how long the model lasts in the environment\n",
    "-particularly important in environments that don't have a fixed environment length - will see more of this in other examples.\n",
    "\n",
    "If model is not performing well: \n",
    "-Training for longer\n",
    "-Hyperparameter tuning -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a callback to the training stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies\n",
    "\n",
    "EvalCallback: the callback that runs during our training\n",
    "\n",
    "\n",
    "StopTrainingOnRewardThreshold: think of this as a checker. Once model passes a reward threshold, this stops the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold = 200, verbose = 1) #stops training once you reach a reward theshold. \n",
    "eval_callback = EvalCallback(env, callback_on_new_best = stop_callback, eval_freq = 10000, \n",
    "                            best_model_save_path= save_path, verbose = 1)\n",
    "# pass through environment, the call back to run on the best model (everytime theres a new best model, stop_callback runs ), how many times to evaluate, \n",
    "# best_model_save_path: the eval callback can save the model everytime there is a new best model.\n",
    "#right now, save_path is our best model \n",
    "\n",
    "#save_path is our best model:\n",
    "#so after 10000 evaluations, it will double check if it has passes the reward threshold. And if it has, training will stop and the model will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these are established, we need to associate these with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_9\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2539 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1702        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007660222 |\n",
      "|    clip_fraction        | 0.079       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.000192   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.18        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 52.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1523        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012586094 |\n",
      "|    clip_fraction        | 0.087       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0881      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 37.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1450        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008818191 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 49.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=139.20 +/- 13.47\n",
      "Episode length: 139.20 +/- 13.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 139         |\n",
      "|    mean_reward          | 139         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011182314 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 61          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1366  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1324        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006329741 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 65.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1249        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009203963 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.7        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 60.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1236        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005072883 |\n",
      "|    clip_fraction        | 0.023       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00345    |\n",
      "|    value_loss           | 66.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1229        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004139031 |\n",
      "|    clip_fraction        | 0.0457      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.3        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00379    |\n",
      "|    value_loss           | 81.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037348925 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.571       |\n",
      "|    explained_variance   | 0.12         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 77.1         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00431     |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 200.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2875b1c2988>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps = 20000, callback = eval_callback)\n",
    "# in this process, best_model actually gets saved. check Training/Saved Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives you a lot more control when training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change policy \n",
    "\n",
    "What if we wanted to use a different neural network for CartPole?\n",
    "\n",
    "To change policy, specify a new neural network architecture. We need to specify a network artchitecture for the custom actor and the value function => changing the amount of units and number of layers in the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New NN architecture defined for custom actor and value function. I don't understand the notation\n",
    "\n",
    "net_arch = [dict(pi=[128,128,128,128], vf = [128,128,128,128])]\n",
    "\n",
    "#pi: for our custom actor: 4 layers, 128 layers in each layer\n",
    "\n",
    "# Question: it is a separate new neural network for custom layer and value function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path, policy_kwargs={'net_arch':net_arch})\n",
    "\n",
    "# policy_kwargs: specifies a new neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_10\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1841 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1139        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014287243 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.00132    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.44        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1016        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016995562 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 965         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013272071 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    value_loss           | 41.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016017923 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 38.6        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 897   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006281199 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00313    |\n",
      "|    value_loss           | 61.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 867         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006320704 |\n",
      "|    clip_fraction        | 0.0876      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.066       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.5        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00781    |\n",
      "|    value_loss           | 85.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007846989 |\n",
      "|    clip_fraction        | 0.043       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.0531      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 52.5        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00311    |\n",
      "|    value_loss           | 75          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 860          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029986068 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.43         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.4         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.000254     |\n",
      "|    value_loss           | 54.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028922842 |\n",
      "|    clip_fraction        | 0.018        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.601        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.5         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 81.9         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 840   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2875b1c2a88>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps = 20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
