{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36fb29b",
   "metadata": {},
   "source": [
    "# MC Control - Blackjack Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b9f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_Q(state_action_freq, state_action_Q, total_return): \n",
    "    \n",
    "    \n",
    "    temp_diff = total_return - state_action_Q               #state_action_return\n",
    "    updated_Q = state_action_Q + (1/state_action_freq) * (temp_diff)\n",
    "    \n",
    "\n",
    "    return updated_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57044e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control():     \n",
    "    \n",
    "    sa_rewards = {}\n",
    "    sa_Q= {}\n",
    "    sa_optimQ = {}\n",
    "    num_state_val_pair = {}\n",
    "    s_ar = {}\n",
    "    Q = 0.5\n",
    "    \n",
    "    eplison = 0.5\n",
    "    \n",
    "    for i in range(num_eps):\n",
    "        \n",
    "        #generate new dealer and player every game. so, generate an episode.\n",
    "    \n",
    "        is_terminal = False # means that game is NOT over. \n",
    "        \n",
    "        \n",
    "        while not is_terminal: \n",
    "\n",
    "            action = random.randint(0,1) # epsilon policy\n",
    "            \n",
    "            n_state, reward, is_terminal, info = env.step(action)\n",
    "            #print((state, action))\n",
    "            episode_sa.append((state, action))\n",
    "            rewards_sa.append(reward)\n",
    "            if state not in s_ar.keys():\n",
    "                s_ar[state] = []\n",
    "            s_ar[state].append((action, reward))\n",
    "            \n",
    "                \n",
    "            \n",
    "        is_terminal = False\n",
    "        \n",
    "        #first occurrence logic\n",
    "        visited_sa = []\n",
    "        for i in range(len(episode_sa)): \n",
    "            \n",
    "            if episode_sa[i] not in visited_sa: \n",
    "                \n",
    "                # accumulate award after first occurrence. \n",
    "                sa_rewards[episode_sa[i]] = []\n",
    "                sa_rewards[episode_sa[i]].append(sum(reward_sa[i:]))\n",
    "                \n",
    "                # start the count for the number of times the first occurence shows up\n",
    "                num_state_value_pair[episode_sa[i]] = 1\n",
    "                \n",
    "                # fresh Q value and dict list value\n",
    "                \n",
    "                #sa_Q[episode_sa[i]] = [0]\n",
    "                \n",
    "                # add to visited list\n",
    "                visited_sa.append(episode_sa[i])\n",
    "            else:\n",
    "                 num_state_value_pair[episode_sa[i]] += 1\n",
    "                \n",
    "           #is this right?\n",
    "            \n",
    "            Q = incremental_Q(num_state_value_pair[episode_sa[i]], Q, sa_rewards[episode_sa[i]])\n",
    "            sa_Q[episode_sa[i]].append(Q)\n",
    "            \n",
    "        #sooo i also need to keep track of the number of the distinct states and correspond those to their actions, \n",
    "        #and then correspond those to their Q values. Ugh.\n",
    "        \n",
    "        # e-greedy!!\n",
    "         \n",
    "        for state in s_ar: \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78689db",
   "metadata": {},
   "source": [
    "# Following Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9795093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import json\n",
    "#from plot_utils import plot_policy, plot_win_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9afa37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6, False)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "state = env.reset()\n",
    "action_space = env.action_space.n\n",
    "\n",
    "print(state)\n",
    "print(action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174bdc4",
   "metadata": {},
   "source": [
    "State: First value is the sum of the player's first two cards, Second value is the Dealer Card that is showing, third value is if there is a usable ace or not. There is a usable ace when you draw an ace that can be used as an \"11\" to win the game\n",
    "\n",
    "\n",
    "\n",
    "Action Space: You get 2 actions, hit or stick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373efcf",
   "metadata": {},
   "source": [
    "The Q function maps states (which are three tuples) to two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf17c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = defaultdict(lambda: np.zeros(2))\n",
    "\n",
    "# how does this work? \n",
    "\n",
    "Q[env.reset()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd252c",
   "metadata": {},
   "source": [
    "index 0 is the Q value for getting a hit, index 1 is the Q value for getting a stick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da04168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "num_episodes = 1000000\n",
    "epsilon = 1\n",
    "epsilon_min = 0.05\n",
    "gamma = 1\n",
    "alpha = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55dc4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_control(num_episodes, epsilon, epsilon_min, alpha, gamma = 1): \n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(2))\n",
    "    \n",
    "    rewards_all_episodes = [] # for calculating the winrate\n",
    "    \n",
    "    for i in range(1, num_episodes + 1):\n",
    "        \n",
    "        #update epsilon values. this is how the epsilon value decays\n",
    "        epsilon = max(epsilon_min, epsilon * 0.999999) # in every iteration of this for loop, epsilon decays by 1-0.999999 percent\n",
    "        #so, after many episodes, this epsilon value is going to get so low (closer to epsilon_min), that we be explorting much more than er\n",
    "        #explore. however, epsilon_min is there so we are still guaranteed some degree of exploration\n",
    "        \n",
    "        if i % 1000 == 0: \n",
    "            print(\"\\rEpisode {}/{}.\".format(i, num_episodes), end = \"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        #need to write function to generate episodes\n",
    "            #action regards to the epsilon greedy policy (also need function for epsilon_greedy)\n",
    "            # also return states, actions, and rewards\n",
    "        experience = generate_episode(Q, epsilon) #one game\n",
    "        \n",
    "        states, actions, rewards = zip(*experience) # give us states actions and rewards in separate lists\n",
    "        rewards = np.array(rewards)\n",
    "        rewards_all_episodes.append(sum(rewards))\n",
    "        \n",
    "       \n",
    "        \n",
    "        #need to loop over the timesteps for that episode or experience\n",
    "        \n",
    "        for i, state in enumerate(states):\n",
    "            discounts = [gamma ** i for i in range(len(rewards[i:]))]\n",
    "            returns = sum(rewards[i:] * discounts)\n",
    "            Q[state][actions[i]] += alpha * (returns -  Q[state][actions[i]])\n",
    "            # alpha is a constant term that leads the loss in direction of the update value and to make the Q value\n",
    "            # more accurate over time\n",
    "            policy = dict((state, np.argmax(q_value)) for state, q_value in Q.items())\n",
    "            \n",
    "    return Q, policy, rewards_all_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29460f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(Q, epsilon):\n",
    "    \n",
    "   \n",
    "    state = env.reset() # everytime we generate an episode, we need to do this so that we can intialize a random state \n",
    "    # as the games starting point\n",
    "    episode = [] # need to keep track of the states, actions, and rewards from the episode (represent as a tuple)\n",
    "    action =  epsilon_greedy_policy(Q, state, epsilon) #defined with epsilon greedy policy\n",
    "    \n",
    "    while True: # keep running until the game ends (while done is false)\n",
    "    \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state # the current state is going to turn into the next state, and so on. did not have this in my original function\n",
    "        episode.append((state, action, reward)) # i had this a dictionary with s,a as the key and the reward as the dictionary\n",
    "        \n",
    "        if done == True : #why do we need this?\n",
    "            break\n",
    "        \n",
    "    return episode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4664bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    \n",
    "    # in this policy, we take the random action epsilon * 100 percent of the time. The rest of the time (1-e) we are going to \n",
    "    #choose the action that maximized the Q value. This is the argmax of the Q function for some state. Select the action that has the highest value\n",
    "    \n",
    "        probs = np.zeros(2)\n",
    "        optimal_action = np.argmax(Q[state]) # will either be a zero or a 1\n",
    "        sub_optimal_action = np.abs(optimal_action - 1)# the OTHER action, (lower Q value). if optimal value is 0, sub is 1, and vice verse\n",
    "        \n",
    "        \n",
    "        probs[optimal_action] = (1 - epsilon) + (epsilon/2)\n",
    "        probs[sub_optimal_action] = (epsilon/2)\n",
    "        \n",
    "        # choose an action based on the probabilities assigned\n",
    "        \n",
    "        action = np.random.choice(np.arange(2), p = probs) # choose the choices based on the probabilities and the indexes\n",
    "        # for the optimal and sub action\n",
    "    \n",
    "        # how do we know the other in which the optimal and sub will be in probs?\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa15c91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((6, 5, False), 1, 0.0),\n",
       " ((12, 5, False), 1, 0.0),\n",
       " ((14, 5, False), 1, 0.0),\n",
       " ((18, 5, False), 1, 0.0),\n",
       " ((21, 5, False), 1, 0.0),\n",
       " ((31, 5, False), 1, -1.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_episode(Q, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e609f",
   "metadata": {},
   "source": [
    "reward is going to be zero when the episode hasn't terminated yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f4a81c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14000/1000000."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-1f12ad16484e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_all_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonte_carlo_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-8287b0de7304>\u001b[0m in \u001b[0;36mmonte_carlo_control\u001b[0;34m(num_episodes, epsilon, epsilon_min, alpha, gamma)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# alpha is a constant term that leads the loss in direction of the update value and to make the Q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# more accurate over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_all_episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-8287b0de7304>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# alpha is a constant term that leads the loss in direction of the update value and to make the Q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# more accurate over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_all_episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q, policy, rewards_all_episodes = monte_carlo_control(num_episodes, epsilon, epsilon_min, alpha, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25845737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001a81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
