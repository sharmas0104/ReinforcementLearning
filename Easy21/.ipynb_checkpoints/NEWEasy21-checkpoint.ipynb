{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy 21 Environment - A Python Class\n",
    "\n",
    "Object of the game is to beat the dealer in any situation\n",
    "\n",
    "im guessing the game ends when the dealer sticks and the player score is less than the dealer score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Easy21():\n",
    "    \n",
    "    \n",
    "    # need to make sure that you can only enter either dealer of player, not both \n",
    "    def __init__(self, dealer = True, player = False): #dealer = False, player = False):\n",
    "      \n",
    "        # check whether the opponent is a dealer or a player. \n",
    "        #This is improtant because the functionality changes based on this information. \n",
    "        self.dealer = dealer\n",
    "        self.player = player\n",
    "        \n",
    "        # all initial settings are set to 0 or indicate that nothing has happened\n",
    "        self.card_color = 'nothing dealt'\n",
    "        self.card = 0\n",
    "        self.card_score = 0\n",
    "        self.num_cards_dealt = 0\n",
    "        self.first_card = \"nothing dealt\"\n",
    "        self.reward = 0;\n",
    "    \n",
    "    def deal_card(self): \n",
    "    \n",
    "        import random\n",
    "        \n",
    "        card_color_num = 0 \n",
    "        \n",
    "        if self.num_cards_dealt != 0:\n",
    "            \n",
    "            card_color_num = random.randint(1,9);\n",
    "            \n",
    "            \n",
    "        self.card = random.randint(1,10)\n",
    "        \n",
    "        \n",
    "        if self.num_cards_dealt == 0:\n",
    "            self.card_color = 'black'\n",
    "            self.card_score += self.card\n",
    "\n",
    "        elif (card_color_num <= 3): \n",
    "\n",
    "            self.card_color = 'red'\n",
    "            self.card_score -= self.card\n",
    "        else:\n",
    "\n",
    "            self.card_color = 'black'\n",
    "            self.card_score += self.card\n",
    "            \n",
    "        if self.first_card == \"nothing dealt\": \n",
    "            self.first_card = self.card_color + ' ' + str(self.card)      \n",
    "        self.num_cards_dealt += 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_card_state(self):\n",
    "        \n",
    "        print(self.card_color + ' ' + str(self.card))\n",
    "        \n",
    "            \n",
    "   # def play(self):\n",
    "        \n",
    "        #if self.dealer: \n",
    "            \n",
    "           # while self.card_score < 17:\n",
    "               # self.hit()\n",
    "               # if self.card_score > 21 or self.card_score < 1:\n",
    "                   # reward = -1\n",
    "                   # print('lose')\n",
    "            \n",
    "       # if (self.card_score > 21 or self.card_score < 1):\n",
    "\n",
    "                #reward = -1\n",
    "               # print('lose')\n",
    "       \n",
    "                \n",
    "                \n",
    "        \n",
    "    def hit(self):\n",
    "        \n",
    "        self.deal_card()\n",
    "        #print('hit')\n",
    "    \n",
    "    #def stick(self):\n",
    "        \n",
    "        #print('stick')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step function pseudo code\n",
    "\n",
    "\n",
    "def step(dealer, player, dealer_first_card, player_sum, a):\n",
    "    \n",
    "    \n",
    "    \n",
    "    reward = 0; \n",
    "    if a == \"hit\": \n",
    "        \n",
    "        player.hit()\n",
    "        player_sum = player.card_score\n",
    "    \n",
    "        \n",
    "        #if the player hits and the card score becomes in over 21 or under 1, the lose, getting a -1 reward\n",
    "        if (player.card_score > 21 or player.card_score < 1):\n",
    "            reward = -1\n",
    "            player.reward = -1\n",
    "        \n",
    "        # if after hitting the score = 21, then they win, getting +1 reward. \n",
    "        if player.card_score == 21: \n",
    "            reward = 1\n",
    "            player.reward = 1\n",
    "        \n",
    "        #if neither of these above events happen, the reward stays 0. \n",
    "        successor_state = (dealer_first_card, player_sum)\n",
    "        return successor_state, reward\n",
    "            \n",
    "    # this is when the player decides to \"stick\". This means the dealer starts taking turns. The dealer has to keep \n",
    "    #hitting until the sum of his cards is 17 or greater\n",
    "    else: \n",
    "    #The dealer has to keep \n",
    "    #hitting until the sum of his cards is 17 or greater\n",
    "        while dealer.card_score < 17:\n",
    "            dealer.hit()\n",
    "            dealer.get_card_state()\n",
    "            \n",
    "        \n",
    "            print(dealer.card_score)\n",
    "            # if at any point while he is pulling cards the card sum goes under 1, the player wins, getting +1 reward\n",
    "            if dealer.card_score < 1:\n",
    "                reward = 1\n",
    "                player.reward = 1\n",
    "                successor_state = (dealer_first_card, player_sum)\n",
    "                return successor_state,reward\n",
    "        \n",
    "        #if after the dealer card sum is 21 after exceeding 17, the player wings, getting +1 reward. \n",
    "        if dealer.card_score > 21:\n",
    "            reward = 1\n",
    "            player.reward = 1\n",
    "            successor_state = (dealer_first_card, player_sum)\n",
    "            return successor_state, reward\n",
    "        # if the dealer is still within conditions, the scores of the dealer and player are compared. if the dealer's score\n",
    "        #is greater than the player's score, then the dealer wins, and vice versa. \n",
    "        else: \n",
    "            if dealer.card_score > player_sum:\n",
    "                player.reward = -1\n",
    "                successor_state = (dealer_first_card, player_sum)\n",
    "                return successor_state, reward\n",
    "            elif dealer.card_score == player_sum:\n",
    "                successor_state = (dealer_first_card, player_sum)\n",
    "                return successor_state, reward\n",
    "            else: \n",
    "                successor_state = (dealer_first_card, player_sum)\n",
    "                player.reward += 1\n",
    "                return successor_state, reward    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out the Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'black 4'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dealer = Easy21()\n",
    "dealer.deal_card()\n",
    "dealer.first_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black 1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "player = Easy21(dealer = False, player = True)\n",
    "player.deal_card()\n",
    "print(player.first_card)\n",
    "print(player.card_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('black 4', 2), 0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(dealer.first_card, player.card_score, \"hit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('black 4', 0), -1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(dealer.first_card, player.card_score, \"hit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('black 7', 18), 0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(dealer.first_card, player.card_score, \"stick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like I have an okay enough implementation of the Step Function. But I'm not sure I'm returning the right things. It doesn't make sense to me how "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Policy Evaluation - Model Free Prediction\n",
    "Estimating the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Visit Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want to worry about control right now. \n",
    "1. Add the functionality that indicates when a game actually terminates. \n",
    "2. Initial value function to 0\n",
    "3. Generate random actions until the game terminates (until the reward does not equal 0)\n",
    "4. when an episode terminates, generate a new player and dealer. \n",
    "5. For each episode, count how many times a state action pair occurs, and then get the reward that comes after it. \n",
    "6. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_action():\n",
    "    \n",
    "    import random\n",
    "    random_action = ''\n",
    "    random_num = random.randint(1,2)\n",
    "    if random_num == 1: \n",
    "        random_action = \"hit\"\n",
    "    else: \n",
    "        random_action = \"stick\"\n",
    "    return random_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc():\n",
    "    \n",
    "    #dictionaries\n",
    "    \n",
    "    num_state_val_pair = {} # stores the number of time each state shows up\n",
    "    state_val_pair_return = {} # stored the accumulated reward and is incremented every time a reward is achieved from that state\n",
    "    state_val_pair_Q = {} #stores the action value for each action value pair. \n",
    "    \n",
    "    \n",
    "    action_value_function = 0\n",
    "    num_episodes = 50\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #generate new dealer and player every game\n",
    "        dealer = Easy21()\n",
    "        dealer.deal_card()\n",
    "        player = Easy21(dealer = False, player = True)\n",
    "        player.deal_card()\n",
    "        \n",
    "        #generate a random action with a 50/50 policy. function rand_action()\n",
    "\n",
    "        #general count for the reward accumulated. \n",
    "        accumulated_reward = 0\n",
    "        is_terminal = False #meaning that the game is NOT over. an episode ends when the game ends. \n",
    "        \n",
    "      \n",
    "        while not is_terminal:  #while the game is still on, i.e. no one has received an award...\n",
    "            \n",
    "            #if the state and reward that was outputted from the step action is not in this dictionary, that means this is the first time it was visited.\n",
    "            state, step_reward = step(dealer, player, dealer.first_card, player.card_score, rand_action()) #take another action, store the result. \n",
    "            accumulated_reward += step_reward\n",
    "\n",
    "            \n",
    "            if not state in num_state_val_pair.keys(): #(state, step_award)\n",
    "                #make a new key value pair, and make the value 1, signifying that the value has showed up once. \n",
    "                num_state_val_pair[state] = 1 #, step_reward] = 1\n",
    "        \n",
    "                #whatever reward was received is also stored as a new key value pair\n",
    "                state_val_pair_return[state] = step_reward #, step_reward] = step_reward#what is was before: step_reward\n",
    "                # right now, the value is just initialized to 0. \n",
    "                state_val_pair_Q[(state)] = 0#, step_reward)] = 0\n",
    "                \n",
    "           \n",
    "          \n",
    "            for key in state_val_pair_return.keys(): \n",
    "                # seriously...omg. \n",
    "                state_val_pair_return[key] += accumulated_reward #what is was before: step_reward #add to the reward\n",
    "                \n",
    "                \n",
    "            if step_reward != 0: \n",
    "                is_terminal = True\n",
    "    \n",
    "    # update Q dictionary (the slow way)   \n",
    "    for key in state_val_pair_return.keys(): \n",
    "        state_val_pair_Q[key] = state_val_pair_return[key]/num_state_val_pair[key]\n",
    "        \n",
    "        \n",
    "      \n",
    "    #once all of the episodes run, you have to calculate the q values for each state/action pair \n",
    "    #in your dictonaries. but what about incremental mean?\n",
    "    #okay, do the first way, and then try incremental mean. AIHFOISDHFF\n",
    "    \n",
    "    return state_val_pair_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red 3\n",
      "2\n",
      "red 4\n",
      "-2\n",
      "red 2\n",
      "-1\n",
      "black 1\n",
      "10\n",
      "black 3\n",
      "13\n",
      "black 3\n",
      "16\n",
      "black 3\n",
      "19\n",
      "black 1\n",
      "4\n",
      "black 9\n",
      "13\n",
      "black 7\n",
      "20\n",
      "black 6\n",
      "14\n",
      "black 10\n",
      "24\n",
      "red 1\n",
      "1\n",
      "black 9\n",
      "10\n",
      "red 7\n",
      "3\n",
      "red 6\n",
      "-3\n",
      "red 8\n",
      "-7\n",
      "red 8\n",
      "-5\n",
      "red 7\n",
      "-3\n",
      "black 1\n",
      "3\n",
      "red 9\n",
      "-6\n",
      "red 9\n",
      "-2\n",
      "red 8\n",
      "-1\n",
      "black 3\n",
      "12\n",
      "black 4\n",
      "16\n",
      "red 7\n",
      "9\n",
      "black 4\n",
      "13\n",
      "red 4\n",
      "9\n",
      "red 9\n",
      "0\n",
      "black 4\n",
      "7\n",
      "black 8\n",
      "15\n",
      "black 9\n",
      "24\n",
      "red 8\n",
      "-5\n",
      "red 3\n",
      "5\n",
      "black 6\n",
      "11\n",
      "red 4\n",
      "7\n",
      "red 8\n",
      "-1\n",
      "black 9\n",
      "18\n",
      "black 4\n",
      "8\n",
      "black 10\n",
      "18\n",
      "black 8\n",
      "17\n",
      "red 8\n",
      "-2\n",
      "red 10\n",
      "-7\n",
      "black 6\n",
      "13\n",
      "black 3\n",
      "16\n",
      "black 2\n",
      "18\n",
      "black 4\n",
      "8\n",
      "red 1\n",
      "7\n",
      "black 4\n",
      "11\n",
      "black 7\n",
      "18\n",
      "red 2\n",
      "3\n",
      "red 6\n",
      "-3\n",
      "black 5\n",
      "13\n",
      "black 4\n",
      "17\n",
      "red 9\n",
      "-6\n",
      "black 2\n",
      "11\n",
      "red 5\n",
      "6\n",
      "black 5\n",
      "11\n",
      "black 7\n",
      "18\n",
      "red 2\n",
      "7\n",
      "black 8\n",
      "15\n",
      "black 7\n",
      "22\n",
      "black 5\n",
      "13\n",
      "black 3\n",
      "16\n",
      "black 2\n",
      "18\n",
      "red 9\n",
      "-7\n",
      "black 9\n",
      "17\n",
      "black 1\n",
      "4\n",
      "black 7\n",
      "11\n",
      "black 4\n",
      "15\n",
      "black 10\n",
      "25\n",
      "black 1\n",
      "5\n",
      "red 7\n",
      "-2\n",
      "black 2\n",
      "9\n",
      "black 2\n",
      "11\n",
      "black 8\n",
      "19\n",
      "black 4\n",
      "5\n",
      "red 5\n",
      "0\n",
      "black 8\n",
      "14\n",
      "black 7\n",
      "21\n",
      "black 6\n",
      "16\n",
      "black 2\n",
      "18\n",
      "black 4\n",
      "13\n",
      "black 10\n",
      "23\n",
      "black 2\n",
      "10\n",
      "black 6\n",
      "16\n",
      "black 2\n",
      "18\n",
      "red 6\n",
      "2\n",
      "black 7\n",
      "9\n",
      "black 9\n",
      "18\n",
      "black 8\n",
      "16\n",
      "black 8\n",
      "24\n",
      "black 10\n",
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('black 5', 3): 3.0,\n",
       " ('black 1', 11): 1.0,\n",
       " ('black 1', 14): 1.0,\n",
       " ('black 9', 8): 0.0,\n",
       " ('black 9', 13): 0.0,\n",
       " ('black 9', 20): 0.0,\n",
       " ('black 9', 26): -1.0,\n",
       " ('black 3', 4): 1.0,\n",
       " ('black 3', -2): 0.0,\n",
       " ('black 8', 7): 3.0,\n",
       " ('black 2', 6): 2.0,\n",
       " ('black 1', 4): 0.0,\n",
       " ('black 3', 8): -1.0,\n",
       " ('black 4', 7): -1.0,\n",
       " ('black 6', -1): -4.0,\n",
       " ('black 2', 2): -2.0,\n",
       " ('black 7', 2): -2.0,\n",
       " ('black 7', 5): -4.0,\n",
       " ('black 9', 2): -4.0,\n",
       " ('black 3', 6): -5.0,\n",
       " ('black 3', 10): -6.0,\n",
       " ('black 8', 1): -7.0,\n",
       " ('black 3', 12): -9.0,\n",
       " ('black 3', 5): -9.0,\n",
       " ('black 3', 13): -9.0,\n",
       " ('black 3', 23): -10.0,\n",
       " ('black 9', 10): -8.0,\n",
       " ('black 9', 14): -8.0,\n",
       " ('black 9', 22): -9.0,\n",
       " ('black 4', 5): -7.0,\n",
       " ('black 4', 0): -8.0,\n",
       " ('black 6', 13): -6.0,\n",
       " ('black 6', 19): -6.0,\n",
       " ('black 6', 24): -7.0,\n",
       " ('black 9', 4): -5.0,\n",
       " ('black 9', 11): -5.0,\n",
       " ('black 9', 3): -5.0,\n",
       " ('black 9', -3): -6.0,\n",
       " ('black 6', 1): -3.0,\n",
       " ('black 3', 11): -5.0,\n",
       " ('black 9', 0): -7.0,\n",
       " ('black 7', 6): -5.0,\n",
       " ('black 7', 1): -5.0,\n",
       " ('black 7', 11): -5.0,\n",
       " ('black 7', 18): -5.0,\n",
       " ('black 7', 19): -5.0,\n",
       " ('black 7', 27): -6.0,\n",
       " ('black 4', 12): -4.0,\n",
       " ('black 4', 17): -4.0,\n",
       " ('black 4', 26): -5.0,\n",
       " ('black 8', 12): -4.0,\n",
       " ('black 8', 20): -4.0,\n",
       " ('black 8', 17): -4.0,\n",
       " ('black 8', 16): -4.0,\n",
       " ('black 8', 18): -4.0,\n",
       " ('black 8', 15): -4.0,\n",
       " ('black 8', 8): -4.0,\n",
       " ('black 8', 9): -4.0,\n",
       " ('black 8', 27): -5.0,\n",
       " ('black 9', 9): -4.0,\n",
       " ('black 9', 18): -4.0,\n",
       " ('black 9', 25): -5.0,\n",
       " ('black 6', 8): -3.0,\n",
       " ('black 6', 0): -4.0,\n",
       " ('black 8', 3): -3.0,\n",
       " ('black 8', 13): -3.0,\n",
       " ('black 8', 6): -3.0,\n",
       " ('black 8', 11): -3.0,\n",
       " ('black 8', 19): -3.0,\n",
       " ('black 2', 11): -2.0,\n",
       " ('black 8', 21): -2.0,\n",
       " ('black 2', 5): -4.0,\n",
       " ('black 2', 0): -5.0,\n",
       " ('black 7', -1): -4.0,\n",
       " ('black 2', -1): -3.0,\n",
       " ('black 3', 2): 0.0,\n",
       " ('black 4', 16): -2.0,\n",
       " ('black 4', 19): -2.0,\n",
       " ('black 7', 3): -3.0,\n",
       " ('black 7', 17): -3.0,\n",
       " ('black 7', 22): -4.0,\n",
       " ('black 1', 9): -1.0,\n",
       " ('black 6', 7): -3.0,\n",
       " ('black 6', 15): -3.0,\n",
       " ('black 6', 25): -4.0,\n",
       " ('black 10', 8): -2.0,\n",
       " ('black 10', 4): -2.0,\n",
       " ('black 10', -4): -3.0,\n",
       " ('black 9', 6): 0.0,\n",
       " ('black 8', 23): -3.0,\n",
       " ('black 8', 4): -1.0,\n",
       " ('black 8', 10): -1.0,\n",
       " ('black 8', -1): -2.0,\n",
       " ('black 8', -2): -2.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_visit_mc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the First Prediction MC Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Mean Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after one episode ends, i'm thinking ill have to loop through the dictionaries so that \n",
    "# i can get the (S,A) returns and frequencies, call incremental_avg(), and then \n",
    "# fill the Q dictionary. \n",
    "\n",
    "#might need to premake Q dict to have zeroes in it as states are seen. That is not hard. \n",
    "\n",
    "def incremental_Q(state_action_freq, state_action_Q, total_return): \n",
    "    \n",
    "    \n",
    "    temp_diff = total_return - state_action_Q               #state_action_return\n",
    "    updated_Q = state_action_Q + (1/state_action_freq) * (temp_diff)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return updated_Q\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it looks like you have most of the code already, you just need a mechanism (basically just a loop) that does E Greedy Exploration. \n",
    "\n",
    "This is similar to MC prediction but not the same, so you probably want to stay away from calling the e_greedy policy function in the predict() method\n",
    "\n",
    " Q[s][a] = Q[s][a] + alpha*(G - Q[s][a]) This is the kind of think you'll need, I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-dfb812656863>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-dfb812656863>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# test test is my comment working. \n",
    "\n",
    "# n = number of times a state action pair has been visited. \n",
    "\n",
    "# initialize Q dictionaries: \n",
    "\n",
    "state_reward = {} # map the state to the reward\n",
    "state_action\n",
    "\n",
    "def e_greedy(n,e):\n",
    "    \n",
    "    import random \n",
    "    \n",
    "    n_0 = 100\n",
    "    step_size = 1/n # learning rate for the algorithm. \n",
    "    e_t = n_0/ (n_0 + n) # this is how we decide to explore or exploit\n",
    "    \n",
    "    if random.random() > e_t :\n",
    "        # exploit\n",
    "        \n",
    "    else: \n",
    "        #explore\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step function pseudo code\n",
    "\n",
    "\n",
    "def step(dealer, player, dealer_first_card, player_sum, a):\n",
    "    \n",
    "    \n",
    "    \n",
    "    reward = 0; \n",
    "    #if a == \"hit\": \n",
    "    if a == 0; \n",
    "        \n",
    "        player.hit()\n",
    "        player_sum = player.card_score\n",
    "    \n",
    "        \n",
    "        #if the player hits and the card score becomes in over 21 or under 1, the lose, getting a -1 reward\n",
    "        if (player.card_score > 21 or player.card_score < 1):\n",
    "            reward = -1\n",
    "            player.reward = -1\n",
    "        \n",
    "        # if after hitting the score = 21, then they win, getting +1 reward. \n",
    "        if player.card_score == 21: \n",
    "            reward = 1\n",
    "            player.reward = 1\n",
    "        \n",
    "        #if neither of these above events happen, the reward stays 0. \n",
    "        successor_state = (dealer_first_card, player_sum)\n",
    "        return successor_state, a, reward\n",
    "            \n",
    "    # this is when the player decides to \"stick\". This means the dealer starts taking turns. The dealer has to keep \n",
    "    #hitting until the sum of his cards is 17 or greater\n",
    "    else: \n",
    "    #The dealer has to keep \n",
    "    #hitting until the sum of his cards is 17 or greater\n",
    "        while dealer.card_score < 17:\n",
    "            dealer.hit()\n",
    "            dealer.get_card_state()\n",
    "            \n",
    "        \n",
    "            print(dealer.card_score)\n",
    "            # if at any point while he is pulling cards the card sum goes under 1, the player wins, getting +1 reward\n",
    "            if dealer.card_score < 1:\n",
    "                reward = 1\n",
    "                player.reward = 1\n",
    "                successor_state = (dealer_first_card, player_sum)\n",
    "                return successor_state, a, reward\n",
    "        \n",
    "        #if after the dealer card sum is 21 after exceeding 17, the player wings, getting +1 reward. \n",
    "        if dealer.card_score > 21:\n",
    "            reward = 1\n",
    "            player.reward = 1\n",
    "            successor_state = (dealer_first_card, player_sum)\n",
    "            return successor_state, reward\n",
    "        # if the dealer is still within conditions, the scores of the dealer and player are compared. if the dealer's score\n",
    "        #is greater than the player's score, then the dealer wins, and vice versa. \n",
    "        else: \n",
    "            if dealer.card_score > player_sum:\n",
    "                player.reward = -1\n",
    "                successor_state = (dealer_first_card, player_sum)\n",
    "                return successor_state, reward\n",
    "            elif dealer.card_score == player_sum:\n",
    "                successor_state = (dealer_first_card, player_sum)\n",
    "                return successor_state, a, reward\n",
    "            else: \n",
    "                successor_state = (dealer_first_card, player_sum)\n",
    "                player.reward += 1\n",
    "                return successor_state, a, reward    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control():\n",
    "    \n",
    "    sa_rewards = {}\n",
    "    sa_Q= {}\n",
    "    \n",
    "    for i in range(num_eps):\n",
    "        \n",
    "        #generate new dealer and player every game. so, generate an episode.\n",
    "        dealer = Easy21()\n",
    "        dealer.deal_card()\n",
    "        player = Easy21(dealer = False, player = True)\n",
    "        player.deal_card()\n",
    "        \n",
    "        is_terminal = False # means that game is NOT over. \n",
    "        \n",
    "        while not is_terminal:  #while the game is still on, i.e. no one has received an award...\n",
    "        \n",
    "            # at the start of the episode, use the 50/50 policy (rand_action())\n",
    "            state, action, step_reward = step(dealer, player, dealer.first_card, player.card_score, rand_action())\n",
    "            accumulated_reward += step_reward\n",
    "            if not state in sa_rewards.keys(): #(state, action)\n",
    "                # if it is the first time an (s,a) pair is seen, need to make a new value an empty list so we can start tracking accumulated rewards. \n",
    "                sa_rewards[(state, action)] = []\n",
    "    \n",
    "        \n",
    "                #whatever reward was received is also stored as a new key value pair\n",
    "                sa_rewards[(state, action)].append(step_reward) #, step_reward] = step_reward#what is was before: step_reward\n",
    "                # right now, the value is just initialized to 0. \n",
    "                sa_Q[(state, action)] = []\n",
    "                \n",
    "                 # need to also check that this is the first time that the individual state is showing up. I might need a 2D array.\n",
    "                sa_Q[(state, action)].append(0) # the first time, the value is 0 \n",
    "        \n",
    "        \n",
    "                \n",
    "             \n",
    "       # Q [s] [a]\n",
    "       # please please do this at home today...     \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Jack Environment - For Prediction\n",
    "\n",
    "- using the tutorial and code snippets from https://towardsdatascience.com/learning-to-win-blackjack-with-monte-carlo-methods-61c90a52d53e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(env, num_episodes):\n",
    "    \n",
    "    \n",
    "    num_state_val_pair = {} # stores the number of time each state shows up\n",
    "    state_val_pair_return = {} # stored the accumulated reward and is incremented every time a reward is achieved from that state\n",
    "    state_val_pair_Q = {} #stores the action value for each action value pair.\n",
    "    \n",
    "    is_terminal = False\n",
    "\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        #print(i)\n",
    "        \n",
    "        state = env.reset()\n",
    "    \n",
    "        episode_sa = []\n",
    "        rewards_sa = []\n",
    "        \n",
    "        while not is_terminal: \n",
    "\n",
    "            action = random.randint(0,1)\n",
    "            n_state, reward, is_terminal, info = env.step(action)\n",
    "            #print((state, action))\n",
    "            episode_sa.append((state, action))\n",
    "            rewards_sa.append(reward)\n",
    "            \n",
    "        is_terminal = False\n",
    "    \n",
    "            \n",
    "        #if not sa_pair in num_state_val_pair.keys(): \n",
    "\n",
    "                #num_state_val_pair[sa_pair] = 1\n",
    "                #state_val_pair_return[sa_pair] = reward\n",
    "                #state_val_pair_Q[(sa_pair)] = 0\n",
    "                \n",
    "        print(rewards_sa)\n",
    "        #first visit monte carlo method\n",
    "        visited_sa = []\n",
    "        for i in range(len(episode_sa)): \n",
    "            \n",
    "            if episode_sa[i] not in visited_sa: \n",
    "                \n",
    "                state_val_pair_return[episode_sa[i]] = 0\n",
    "                state_val_pair_return[episode_sa[i]] += sum(rewards_sa[i:])\n",
    "                print(state_val_pair_return[episode_sa[i]])\n",
    "                num_state_val_pair[episode_sa[i]] = 1\n",
    "                state_val_pair_Q[episode_sa[i]] = 0\n",
    "                visited_sa.append(episode_sa[i])\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                num_state_val_pair[episode_sa[i]] += 1\n",
    "    \n",
    "            state_val_pair_Q[episode_sa[i]] = incremental_Q(num_state_val_pair[episode_sa[i]],state_val_pair_Q[episode_sa[i]], state_val_pair_return[episode_sa[i]])\n",
    "            #print(state_val_pair_Q[episode_sa[i]])\n",
    "        #update Q dictionary\n",
    "        #for key in state_val_pair_return.keys(): \n",
    "            #state_val_pair_Q[key] = state_val_pair_return[key]/num_state_val_pair[key]\n",
    "            \n",
    "            \n",
    "                \n",
    "        # how do I do this incrementally? \n",
    "            \n",
    "    print(state_val_pair_Q)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0]\n",
      "-1.0\n",
      "[0.0, -1.0]\n",
      "-1.0\n",
      "-1.0\n",
      "[0.0, 1.0]\n",
      "1.0\n",
      "1.0\n",
      "[0.0, -1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[0.0, 1.0]\n",
      "1.0\n",
      "1.0\n",
      "[1.0]\n",
      "1.0\n",
      "[0.0, -1.0]\n",
      "-1.0\n",
      "[0.0, -1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[0.0]\n",
      "0.0\n",
      "[1.0]\n",
      "1.0\n",
      "[0.0, 0.0, 1.0]\n",
      "1.0\n",
      "1.0\n",
      "[0.0, 0.0, 1.0]\n",
      "1.0\n",
      "1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[1.0]\n",
      "1.0\n",
      "[0.0, 1.0]\n",
      "1.0\n",
      "1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[0.0, 0.0, -1.0]\n",
      "-1.0\n",
      "[0.0, -1.0]\n",
      "-1.0\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[1.0]\n",
      "1.0\n",
      "[0.0, 1.0]\n",
      "1.0\n",
      "1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[0.0, -1.0]\n",
      "-1.0\n",
      "-1.0\n",
      "[0.0, 0.0]\n",
      "0.0\n",
      "0.0\n",
      "[1.0]\n",
      "1.0\n",
      "[1.0]\n",
      "1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[1.0]\n",
      "1.0\n",
      "[0.0, -1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[0.0, -1.0]\n",
      "-1.0\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[1.0]\n",
      "1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[1.0]\n",
      "1.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "[0.0]\n",
      "0.0\n",
      "[-1.0]\n",
      "-1.0\n",
      "{((16, 4, False), 1): -1.0, ((21, 10, True), 1): -1.0, ((21, 10, True), 0): 1.0, ((9, 1, False), 1): 1.0, ((9, 1, False), 0): 1.0, ((18, 2, False), 1): -1.0, ((20, 10, False), 1): -1.0, ((20, 10, True), 1): 1.0, ((20, 10, True), 0): 1.0, ((20, 10, False), 0): 0.0, ((12, 4, False), 1): -1.0, ((6, 1, False), 1): -1.0, ((17, 7, True), 0): 0.0, ((20, 6, False), 0): 1.0, ((8, 10, False), 1): 1.0, ((8, 10, False), 0): 1.0, ((5, 9, False), 1): 1.0, ((5, 9, False), 0): 1.0, ((20, 1, False), 1): -1.0, ((13, 3, False), 1): -1.0, ((10, 7, False), 0): -1.0, ((7, 7, False), 0): -1.0, ((17, 9, True), 1): 1.0, ((17, 9, True), 0): 1.0, ((14, 10, False), 0): -1.0, ((10, 10, False), 1): -1.0, ((17, 8, True), 1): -1.0, ((17, 8, True), 0): -1.0, ((9, 9, False), 0): -1.0, ((20, 2, False), 0): 1.0, ((9, 7, False), 1): 1.0, ((9, 7, False), 0): 1.0, ((15, 1, False), 0): -1.0, ((12, 8, False), 1): -1.0, ((17, 10, False), 0): -1.0, ((19, 9, False), 1): -1.0, ((10, 6, False), 0): -1.0, ((13, 10, False), 1): -1.0, ((13, 10, False), 0): -1.0, ((11, 9, False), 1): 0.0, ((11, 9, False), 0): 0.0, ((13, 6, False), 0): 1.0, ((13, 7, False), 0): 1.0, ((7, 1, False), 0): -1.0, ((11, 7, False), 0): 1.0, ((20, 4, False), 1): -1.0, ((20, 9, True), 1): -1.0, ((20, 9, True), 0): -1.0, ((12, 10, False), 0): 1.0, ((16, 2, False), 0): -1.0, ((16, 5, False), 0): -1.0, ((19, 5, False), 0): -1.0, ((20, 8, False), 0): 1.0, ((9, 6, False), 0): -1.0, ((11, 10, False), 0): -1.0}\n"
     ]
    }
   ],
   "source": [
    "mc_prediction(env, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Values\n",
    "\n",
    "Source: https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-understanding-blackjack-strategy-through-monte-carlo-88c9b85194ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_blackjack(V, ax1, ax2):\n",
    "    \n",
    "    player_sum = np.arange(12, 21 + 1)\n",
    "    dealer_show = np.arange(1, 10 + 1)         \n",
    "    usable_ace = np.array([False, True])\n",
    "    state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace)))\n",
    "    for i, player in enumerate(player_sum):\n",
    "    for j, dealer in enumerate(dealer_show):\n",
    "        for k, ace in enumerate(usable_ace):\n",
    "        state_values[i, j, k] = V[player, dealer, ace]\n",
    "    X, Y = np.meshgrid(player_sum, dealer_show)\n",
    "    ax1.plot_wireframe(X, Y, state_values[:, :, 0])   \n",
    "    ax2.plot_wireframe(X, Y, state_values[:, :, 1])\n",
    "    for ax in ax1, ax2:\n",
    "    ax.set_zlim(-1, 1)\n",
    "    ax.set_ylabel(‘player sum’)\n",
    "    ax.set_xlabel(‘dealer sum’)\n",
    "    ax.set_zlabel(‘state-value’)\n",
    "    \n",
    "fig, axes = pyplot.subplots(nrows=2, figsize=(5, 8),subplot_kw={'projection': '3d'})\n",
    "axes[0].set_title('state-value distribution w/o usable ace')\n",
    "axes[1].set_title('state-value distribution w/ usable ace')\n",
    "plot_blackjack(value, axes[0], axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recycled Code Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #at this point, we have generated at least one step of the game. now we have to assess if the game goes on. \n",
    "#honestly, do we even need that first part? it's repeated inside the loop anyway. \n",
    "\n",
    "state, step_reward =  step(dealer.first_card, player.card_score, rand_action()) # one action is taken and the result of that action is stored. \n",
    "        \n",
    "        #if the state and reward that was outputted from the step action is not in this dictionary, that means this is the first time it was visited.\n",
    "        \n",
    "        accumulated_reward += step_reward\n",
    "        \n",
    "        if not (state, step_reward) in num_state_val_pair.keys(): \n",
    "            #make a new key value pair, and make the value 1, signifying that the value has showed up once. \n",
    "            num_state_val_pair[(state, step_reward)] = 1\n",
    "            #whatever reward was received is also stored as a new key value pair\n",
    "            state_val_pair_return[(state, step_reward)] = step_reward       #what is was before: step_reward\n",
    "            # right now, the value is just initialized to 0. \n",
    "            state_val_pair_Q[(state, step_reward)] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
