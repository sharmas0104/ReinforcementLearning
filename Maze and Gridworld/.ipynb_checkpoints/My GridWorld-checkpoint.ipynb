{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.zeros((4,4)) # represents a gridworld in which each state, apart from the terminal state, has a value of 0. \n",
    "terminal_state = [(0,0), (3,3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Check\n",
    "\n",
    "What is value? The sum of all future rewards that can be recieved from a given state s. These values are \"long lasting. Tells us the long term effects of being in a certain state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation for Gridworld \n",
    "\n",
    "What is iterative policy evaluation? Finding the true value function for each state with full knowledge of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the value of each state given the policy\n",
    "def policy_eval(maze_values): \n",
    "  \n",
    "    # WHY CAN'T I DO IT. \n",
    "                            \n",
    "    #action = random.randint(0, (1/policy) - 1)\n",
    "            # 0 = up, 1 = down, 2 = right, 3 = left. maybe put this in a dictionary or something\n",
    "    # make a for loop for the states:\n",
    "    \n",
    "    new_values = maze_values.copy()\n",
    "    \n",
    "    for row in range(len(maze_values[0])): \n",
    "        for col in range(len(maze_values[1])):\n",
    "            reward = -1 # -1 for up, down, left, right\n",
    "            successor_val = 0\n",
    "            initial_state_val = maze_values[row][col]\n",
    "            \n",
    "            if ((row, col) in terminal_state): \n",
    "                new_values[row][col] = 0\n",
    "            else:\n",
    "                # check left\n",
    "                if (col - 1 < 0): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row][col - 1])\n",
    "                # check right\n",
    "                if (col + 1 >= len(maze_values[1])): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row][col + 1])\n",
    "                # check up\n",
    "                if (row - 1 < 0): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row - 1][col])\n",
    "                # check down\n",
    "                if (row + 1 >= len(maze_values[0])): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row+1][col])\n",
    "                \n",
    "                new_values[row][col] = 0.25 * successor_val\n",
    "                \n",
    "            \n",
    "        \n",
    "    return np.array(new_values) \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "maze_values = np.zeros((4,4))\n",
    "\n",
    "print(np.array(maze_values))\n",
    "\n",
    "current_values = maze_values\n",
    "\n",
    "for i in range(1000): \n",
    "    \n",
    "    current_values = policy_eval(current_values)\n",
    "    \n",
    "    \n",
    "print(current_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction\n",
    "\n",
    "How is MC Prediction different than Iterative Policy Evaluation? We do NOT have full knowledge of the environment. We find the true value function through episodes\n",
    "\n",
    "What is First Visit Monte Carlo? Averaging returns for a state after the state's first occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(pos):\n",
    "    \n",
    "    eps = 0.5\n",
    "    #e_greedy(eps, )\n",
    "    action = random.randint(0,3) # random policy\n",
    "    successor_val = 0\n",
    "    successor_state = pos\n",
    "    reward = -1\n",
    "    \n",
    "    if action == 0: # check up\n",
    "        if not (pos[0] - 1 < 0): \n",
    "            successor_state = (pos[0] - 1, pos[1])\n",
    "    if action == 1: # check down\n",
    "        if not (pos[0] + 1 > 3): \n",
    "            successor_state = (pos[0] + 1, pos[1])\n",
    "    if action == 2: # check left\n",
    "        if not (pos[1] - 1 < 0): \n",
    "            successor_state = (pos[0], pos[1] - 1)\n",
    "    if action == 3: # check right\n",
    "        if not (pos[1] + 1 > 3): \n",
    "            successor_state = (pos[0], pos[1] + 1)\n",
    "    \n",
    "    if successor_state in terminal_state: \n",
    "        reward = 0\n",
    "        \n",
    "    if pos in terminal_state:\n",
    "        reward = 0\n",
    "                               \n",
    "    \n",
    "    return pos, action, reward, successor_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(num_iterations): \n",
    "    \n",
    "    #maze_values = np.zeros((4,4))\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    state_rewards = defaultdict(int)\n",
    "    returns = defaultdict(int)\n",
    "    all_states = np.array([])\n",
    "    \n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        \n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "        states_visited = np.array([])\n",
    "        while not is_terminal:\n",
    "            initial_state, action, reward, successor_state = step(initial_state)\n",
    "            states_visited = np.append(states_visited, [initial_state, action, reward, successor_state])\n",
    "            initial_state = successor_state\n",
    "            if initial_state in terminal_state: \n",
    "                states_visited = np.copy(states_visited.reshape(int(len(states_visited)/4), 4))\n",
    "                all_states = states_visited\n",
    "                is_terminal = True\n",
    "        \n",
    "    \n",
    "        for i in range(len(all_states)): \n",
    "      \n",
    "            if all_states[i][0] not in state_rewards.keys(): \n",
    "                state_rewards[all_states[i][0]] = np.sum(all_states[i:, 2])\n",
    "       \n",
    "        \n",
    "   \n",
    "    for state, reward in state_rewards.items(): \n",
    "            \n",
    "        returns[state] = reward / num_iterations\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    return returns, state_rewards\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "            \n",
    "            \n",
    "    # there is still something wrong with this        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrutisharma/anaconda3/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(defaultdict(int,\n",
       "             {(1, 2): -0.015,\n",
       "              (2, 2): -0.014,\n",
       "              (2, 1): -0.013,\n",
       "              (3, 1): -0.01,\n",
       "              (2, 0): -0.008,\n",
       "              (3, 0): -0.007,\n",
       "              (2, 3): 0.0,\n",
       "              (3, 2): 0.0,\n",
       "              (1, 1): -0.003,\n",
       "              (1, 0): -0.002,\n",
       "              (1, 3): -0.004,\n",
       "              (0, 2): -0.002,\n",
       "              (0, 1): -0.001,\n",
       "              (0, 3): -0.027,\n",
       "              (3, 3): -0.002,\n",
       "              (0, 0): 0.0}),\n",
       " defaultdict(int,\n",
       "             {(1, 2): -15,\n",
       "              (2, 2): -14,\n",
       "              (2, 1): -13,\n",
       "              (3, 1): -10,\n",
       "              (2, 0): -8,\n",
       "              (3, 0): -7,\n",
       "              (2, 3): 0,\n",
       "              (3, 2): 0,\n",
       "              (1, 1): -3,\n",
       "              (1, 0): -2,\n",
       "              (1, 3): -4,\n",
       "              (0, 2): -2,\n",
       "              (0, 1): -1,\n",
       "              (0, 3): -27,\n",
       "              (3, 3): -2,\n",
       "              (0, 0): 0}))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns = first_visit_mc(1000)\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-3fb7a385c2c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmaze_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaze_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# construct value grid\n",
    "\n",
    "maze_values = np.zeros((4,4))\n",
    "\n",
    "for i in range(maze_values.shape[0]):\n",
    "    for j in range(maze_values.shape[1]):\n",
    "        maze_values[i][j] = returns[(i,j)]\n",
    "print(maze_values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold up, why aren't the terminal states 0. hhhh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(num_iterations, discount): \n",
    "    \n",
    "    \n",
    "    #maze_values = np.zeros((4,4))\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # at the beginning, all of the states need to have a list of aciton rewards, i\n",
    "    sa_rewards = defaultdict(list)\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    states_visited = []\n",
    "    sa_visited = []\n",
    "    # number of occurences of a state action pair in one episode: \n",
    "    sa_occurences = defaultdict(int)\n",
    "    \n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        \n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "        #sa_visited = np.array([])\n",
    "        \n",
    "        # run until the episode is finished\n",
    "        while not is_terminal:\n",
    "            initial_state, action, reward, successor_state = step(initial_state)\n",
    "            sa = (initial_state, action)\n",
    "            sa_visited.append(sa, reward)\n",
    "            sa_rewards[sa].append(reward)\n",
    "            sa_occurences[sa] += 1 # update counter\n",
    "            initial_state = successor_state\n",
    "            if initial_state in terminal_state: \n",
    "                is_terminal = True\n",
    "    \n",
    "        \n",
    "        for sa, reward in reversed(sa_visited): # iterate backwards\n",
    "            \n",
    "            #sa_rewards[sa_pairs[i]] = sa_rewards[sa_pairs[i]] +  (1 / sa_occurences[sa_pairs[i]]) * (sum(rewards[i:])-sa_rewards[sa_pairs[i]]) \n",
    "            \n",
    "            power = len(sa_visited)\n",
    "            current_return = 0\n",
    "            if sa[0] not in states_visited:\n",
    "            \n",
    "                current_return += (discount ** power) * reward\n",
    "                #sa_rewards[sa] += discount * (1 / sa_occurences[sa]) * (current_return - sa_rewards[sa])\n",
    "                Q[sa[0]][sa[1]] += discount * (1 / sa_occurences[sa]) * (current_return - sa_rewards[sa])\n",
    "                states_visited.append(sa[0])\n",
    "        \n",
    "            power -= 1\n",
    "        \n",
    "        \n",
    "        # e-greedy:\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "            \n",
    "    return sa_rewards\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {((2, 0), 2): [-1, -1], ((2, 0), 0): [-1], ((1, 0), 0): [0]})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_control(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(pos, Q):\n",
    "    \n",
    "    eps = 0.2\n",
    "    action = e_greedy(eps, pos, Q)\n",
    "    successor_val = 0\n",
    "    successor_state = pos\n",
    "    reward = -1\n",
    "    \n",
    "    if action == 0: # check up\n",
    "        if not (pos[0] - 1 < 0): \n",
    "            successor_state = (pos[0] - 1, pos[1])\n",
    "    if action == 1: # check down\n",
    "        if not (pos[0] + 1 > 3): \n",
    "            successor_state = (pos[0] + 1, pos[1])\n",
    "    if action == 2: # check left\n",
    "        if not (pos[1] - 1 < 0): \n",
    "            successor_state = (pos[0], pos[1] - 1)\n",
    "    if action == 3: # check right\n",
    "        if not (pos[1] + 1 > 3): \n",
    "            successor_state = (pos[0], pos[1] + 1)\n",
    "    \n",
    "    #if successor_state in terminal_state: \n",
    "        #reward = 0      \n",
    "        \n",
    "    if pos in terminal_state:\n",
    "        reward = 0\n",
    "                               \n",
    "    \n",
    "    return pos, action, reward, successor_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(eps, pos, Q): \n",
    "    \n",
    "    \n",
    "    \n",
    "    #episilon = the probability of randomly choosing an action\n",
    "    # probablility of choosing the optimal action is 1 - eps.\n",
    "    \n",
    "   \n",
    "    probability = round(random.uniform(0,1), 2)\n",
    "    \n",
    "    if probability <= eps: \n",
    "        action = random.randint(0,3)\n",
    "    else: \n",
    "       \n",
    "        rewards = list(Q[pos])\n",
    "        action = rewards.index(max(rewards))\n",
    "        \n",
    "      \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_control(num_iterations, discount): \n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    episode = []\n",
    "    sa_visited = []\n",
    "    sa_occurences = defaultdict(int)\n",
    "    \n",
    "    \n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "        \n",
    "        while not is_terminal:\n",
    "            \n",
    "            # the action must be chosen with the epsilon greedy policy. \n",
    "            initial_state, action, reward, successor_state = step(initial_state, Q)\n",
    "            sa = (initial_state, action)\n",
    "            episode.append((sa, reward))\n",
    "            sa_occurences[sa] += 1\n",
    "            initial_state = successor_state\n",
    "            if initial_state in terminal_state: \n",
    "                is_terminal = True\n",
    "            \n",
    "        current_return = 0\n",
    "        power = len(episode)\n",
    "        for sa, reward in reversed(episode): \n",
    "                 \n",
    "            if sa not in sa_visited: \n",
    "                    \n",
    "                current_return += (discount ** power) * reward\n",
    "                \n",
    "                Q[sa[0]][sa[1]] += round((1 / sa_occurences[sa]) * (reward - Q[sa[0]][sa[1]]), 2)\n",
    "                    \n",
    "                sa_visited.append(sa)\n",
    "                \n",
    "                power -= 1    \n",
    "    return Q\n",
    "   \n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-06ce42c78489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_visit_mc_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-138-6ed750f61e4e>\u001b[0m in \u001b[0;36mfirst_visit_mc_control\u001b[0;34m(num_iterations, discount)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msa\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msa_visited\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mcurrent_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q = first_visit_mc_control(1000, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -0.25, -0.02, -0.01],\n",
       "       [-0.03, -0.25, -0.33, -0.17],\n",
       "       [-0.03, -0.04, -0.5 , -1.  ],\n",
       "       [-0.05, -0.04, -0.33,  0.  ]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct policy map\n",
    "\n",
    "#Q = first_visit_mc_control(500, 1)\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7fcb6d8a6048>, {(0, 1): array([0., 1., 0., 0.])})\n"
     ]
    }
   ],
   "source": [
    "Q = defaultdict(lambda: np.zeros(4))\n",
    "Q[(0,1)][1] += 1\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
