{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.zeros((4,4)) # represents a gridworld in which each state, apart from the terminal state, has a value of 0. \n",
    "terminal_state = [(0,0), (3,3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Check\n",
    "\n",
    "What is value? The sum of all future rewards that can be recieved from a given state s. These values are \"long lasting. Tells us the long term effects of being in a certain state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation for Gridworld \n",
    "\n",
    "What is iterative policy evaluation? Finding the true value function for each state with full knowledge of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the value of each state given the policy\n",
    "def policy_eval(maze_values): \n",
    "  \n",
    "    \n",
    "    new_values = maze_values.copy()\n",
    "    \n",
    "    for row in range(len(maze_values[0])): \n",
    "        for col in range(len(maze_values[1])):\n",
    "            reward = -1 # -1 for up, down, left, right\n",
    "            successor_val = 0\n",
    "            initial_state_val = maze_values[row][col]\n",
    "            \n",
    "            if ((row, col) in terminal_state): \n",
    "                new_values[row][col] = 0\n",
    "            else:\n",
    "                # check left\n",
    "                if (col - 1 < 0): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row][col - 1])\n",
    "                # check right\n",
    "                if (col + 1 >= len(maze_values[1])): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row][col + 1])\n",
    "                # check up\n",
    "                if (row - 1 < 0): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row - 1][col])\n",
    "                # check down\n",
    "                if (row + 1 >= len(maze_values[0])): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row+1][col])\n",
    "                \n",
    "                new_values[row][col] = 0.25 * successor_val\n",
    "                \n",
    "            \n",
    "        \n",
    "    return np.array(new_values) \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "maze_values = np.zeros((4,4))\n",
    "\n",
    "print(np.array(maze_values))\n",
    "\n",
    "current_values = maze_values\n",
    "\n",
    "for i in range(1000): \n",
    "    \n",
    "    current_values = policy_eval(current_values)\n",
    "    \n",
    "    \n",
    "print(current_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(pos, Q):\n",
    "    \n",
    "    eps = 0.5\n",
    "    action = e_greedy(eps, pos, Q)\n",
    "    successor_val = 0\n",
    "    successor_state = pos\n",
    "    reward = -1\n",
    "    \n",
    "    if action == 0: # check up\n",
    "        if not (pos[0] - 1 < 0): \n",
    "            successor_state = (pos[0] - 1, pos[1])\n",
    "    if action == 1: # check down\n",
    "        if not (pos[0] + 1 > 3): \n",
    "            successor_state = (pos[0] + 1, pos[1])\n",
    "    if action == 2: # check left\n",
    "        if not (pos[1] - 1 < 0): \n",
    "            successor_state = (pos[0], pos[1] - 1)\n",
    "    if action == 3: # check right\n",
    "        if not (pos[1] + 1 > 3): \n",
    "            successor_state = (pos[0], pos[1] + 1)\n",
    "    \n",
    "    #if successor_state in terminal_state: \n",
    "        #reward = 0      \n",
    "        \n",
    "    if pos in terminal_state:\n",
    "        reward = 0\n",
    "                               \n",
    "    \n",
    "    return pos, action, reward, successor_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(eps, pos, Q): \n",
    "    \n",
    "    \n",
    "    \n",
    "    #episilon = the probability of randomly choosing an action\n",
    "    # probablility of choosing the optimal action is 1 - eps.\n",
    "    \n",
    "   \n",
    "    probability = round(random.uniform(0,1), 2)\n",
    "    \n",
    "    if probability <= eps: \n",
    "        action = random.randint(0,3)\n",
    "    else: \n",
    "       \n",
    "        rewards = list(Q[pos])\n",
    "        action = rewards.index(max(rewards))\n",
    "        \n",
    "      \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_control(num_iterations, discount): \n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    episode = []\n",
    "    sa_visited = []\n",
    "    sa_occurences = defaultdict(int)\n",
    "    \n",
    "    \n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "        \n",
    "        while not is_terminal:\n",
    "            \n",
    "            # the action must be chosen with the epsilon greedy policy. \n",
    "            initial_state, action, reward, successor_state = step_control(initial_state, Q)\n",
    "            sa = (initial_state, action)\n",
    "            episode.append((sa, reward))\n",
    "            sa_occurences[sa] += 1\n",
    "            initial_state = successor_state\n",
    "            if initial_state in terminal_state: \n",
    "                is_terminal = True\n",
    "            \n",
    "        current_return = 0\n",
    "        power = len(episode)\n",
    "        for sa, reward in reversed(episode): \n",
    "                 \n",
    "            if sa not in sa_visited: \n",
    "                    \n",
    "                current_return += (discount ** power) * reward\n",
    "                \n",
    "                Q[sa[0]][sa[1]] += round((1 / sa_occurences[sa]) * (reward - Q[sa[0]][sa[1]]), 2)\n",
    "                    \n",
    "                sa_visited.append(sa)\n",
    "                \n",
    "                power -= 1    \n",
    "    return Q\n",
    "   \n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = first_visit_mc_control(2000, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -0.25, -0.5 , -0.06],\n",
       "       [-0.2 , -0.25, -0.25, -0.11],\n",
       "       [-1.  , -1.  , -1.  , -0.25],\n",
       "       [-0.08, -0.2 , -0.5 ,  0.  ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct policy map\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_learning(num_iterations, alpha, discount): \n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "\n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "    \n",
    "        while not is_terminal:\n",
    "\n",
    "            # the action must be chosen with the epsilon greedy policy. \n",
    "            initial_state, action, reward, successor_state = step(initial_state, Q)\n",
    "    \n",
    "            Q[initial_state][action] += alpha * (reward + discount * max(Q[successor_state] - Q[initial_state][action]))\n",
    "        \n",
    "            if initial_state in terminal_state: \n",
    "                is_terminal = True\n",
    "            \n",
    "            initial_state = successor_state\n",
    "                                             \n",
    "    return Q\n",
    "                                        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = td_learning(2000, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -2., -4., -6.],\n",
       "       [-2., -4., -6., -4.],\n",
       "       [-4., -6., -4., -2.],\n",
       "       [-6., -4., -2.,  0.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -2.        , -4.        , -5.99999969],\n",
       "       [-2.        , -4.        , -5.99999319, -4.        ],\n",
       "       [-4.        , -5.99999263, -4.        , -2.        ],\n",
       "       [-5.99999993, -4.        , -2.        ,  0.        ]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = td_learning(1000, 0.5, 0.5)\n",
    "\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -2.        , -4.        , -5.99982938],\n",
       "       [-2.        , -3.99999782, -5.98060658, -4.        ],\n",
       "       [-4.        , -5.98655542, -3.99997676, -2.        ],\n",
       "       [-5.99642811, -3.99999999, -2.        ,  0.        ]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = td_learning(500, 0.5, 0.5)\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -2.        , -4.        , -5.99922538],\n",
       "       [-2.        , -3.99999985, -5.99832279, -4.        ],\n",
       "       [-4.        , -5.99796553, -3.9999983 , -2.        ],\n",
       "       [-5.99998019, -4.        , -2.        ,  0.        ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = td_learning(650, 0.5, 0.5)\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
