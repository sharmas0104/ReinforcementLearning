{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.zeros((4,4)) # represents a gridworld in which each state, apart from the terminal state, has a value of 0. \n",
    "terminal_state = [(0,0), (3,3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Check\n",
    "\n",
    "What is value? The sum of all future rewards that can be recieved from a given state s. These values are \"long lasting. Tells us the long term effects of being in a certain state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation for Gridworld \n",
    "\n",
    "What is iterative policy evaluation? Finding the true value function for each state with full knowledge of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the value of each state given the policy\n",
    "def policy_eval(maze_values): \n",
    "  \n",
    "    \n",
    "    new_values = maze_values.copy()\n",
    "    \n",
    "    for row in range(len(maze_values[0])): \n",
    "        for col in range(len(maze_values[1])):\n",
    "            reward = -1 # -1 for up, down, left, right\n",
    "            successor_val = 0\n",
    "            initial_state_val = maze_values[row][col]\n",
    "            \n",
    "            if ((row, col) in terminal_state): \n",
    "                new_values[row][col] = 0\n",
    "            else:\n",
    "                # check left\n",
    "                if (col - 1 < 0): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row][col - 1])\n",
    "                # check right\n",
    "                if (col + 1 >= len(maze_values[1])): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row][col + 1])\n",
    "                # check up\n",
    "                if (row - 1 < 0): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row - 1][col])\n",
    "                # check down\n",
    "                if (row + 1 >= len(maze_values[0])): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row+1][col])\n",
    "                \n",
    "                new_values[row][col] = 0.25 * successor_val\n",
    "                \n",
    "            \n",
    "        \n",
    "    return np.array(new_values) \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "maze_values = np.zeros((4,4))\n",
    "\n",
    "print(np.array(maze_values))\n",
    "\n",
    "current_values = maze_values\n",
    "\n",
    "for i in range(1000): \n",
    "    \n",
    "    current_values = policy_eval(current_values)\n",
    "    \n",
    "    \n",
    "print(current_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(pos, Q):\n",
    "    \n",
    "    eps = 0.5\n",
    "    action = e_greedy(eps, pos, Q)\n",
    "    successor_val = 0\n",
    "    successor_state = pos\n",
    "    reward = -1\n",
    "    \n",
    "    if action == 0: # check up\n",
    "        if not (pos[0] - 1 < 0): \n",
    "            successor_state = (pos[0] - 1, pos[1])\n",
    "    if action == 1: # check down\n",
    "        if not (pos[0] + 1 > 3): \n",
    "            successor_state = (pos[0] + 1, pos[1])\n",
    "    if action == 2: # check left\n",
    "        if not (pos[1] - 1 < 0): \n",
    "            successor_state = (pos[0], pos[1] - 1)\n",
    "    if action == 3: # check right\n",
    "        if not (pos[1] + 1 > 3): \n",
    "            successor_state = (pos[0], pos[1] + 1)\n",
    "    \n",
    "    #if successor_state in terminal_state: \n",
    "        #reward = 0      \n",
    "        \n",
    "    if pos in terminal_state:\n",
    "        reward = 0\n",
    "                               \n",
    "    \n",
    "    return pos, action, reward, successor_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(eps, pos, Q): \n",
    "    \n",
    "    \n",
    "    \n",
    "    #episilon = the probability of randomly choosing an action\n",
    "    # probablility of choosing the optimal action is 1 - eps.\n",
    "    \n",
    "   \n",
    "    probability = round(random.uniform(0,1), 2)\n",
    "    \n",
    "    if probability <= eps: \n",
    "        action = random.randint(0,3)\n",
    "    else: \n",
    "       \n",
    "        rewards = list(Q[pos])\n",
    "        action = rewards.index(max(rewards))\n",
    "        \n",
    "      \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_control(num_iterations, discount): \n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    episode = []\n",
    "    sa_visited = []\n",
    "    sa_occurences = defaultdict(int)\n",
    "    \n",
    "    \n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "        \n",
    "        while not is_terminal:\n",
    "            \n",
    "            # the action must be chosen with the epsilon greedy policy. \n",
    "            initial_state, action, reward, successor_state = step_control(initial_state, Q)\n",
    "            sa = (initial_state, action)\n",
    "            episode.append((sa, reward))\n",
    "            sa_occurences[sa] += 1\n",
    "            initial_state = successor_state\n",
    "            if initial_state in terminal_state: \n",
    "                is_terminal = True\n",
    "            \n",
    "        current_return = 0\n",
    "        power = len(episode)\n",
    "        for sa, reward in reversed(episode): \n",
    "                 \n",
    "            if sa not in sa_visited: \n",
    "                    \n",
    "                current_return += (discount ** power) * reward\n",
    "                \n",
    "                Q[sa[0]][sa[1]] += round((1 / sa_occurences[sa]) * (reward - Q[sa[0]][sa[1]]), 2)\n",
    "                    \n",
    "                sa_visited.append(sa)\n",
    "                \n",
    "                power -= 1    \n",
    "    return Q\n",
    "   \n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = first_visit_mc_control(2000, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -0.25, -0.5 , -0.06],\n",
       "       [-0.2 , -0.25, -0.25, -0.11],\n",
       "       [-1.  , -1.  , -1.  , -0.25],\n",
       "       [-0.08, -0.2 , -0.5 ,  0.  ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct policy map\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_learning(num_iterations, alpha, discount): \n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "\n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "    \n",
    "        while not is_terminal:\n",
    "\n",
    "            # the action must be chosen with the epsilon greedy policy. \n",
    "            initial_state, action, reward, successor_state = step(initial_state, Q)\n",
    "    \n",
    "            Q[initial_state][action] += alpha * (reward + discount * max(Q[successor_state] - Q[initial_state][action]))\n",
    "        \n",
    "            if initial_state in terminal_state: \n",
    "                is_terminal = True\n",
    "            \n",
    "            initial_state = successor_state\n",
    "                                             \n",
    "    return Q\n",
    "                                        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = td_learning(2000, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -2., -4., -6.],\n",
       "       [-2., -4., -6., -4.],\n",
       "       [-4., -6., -4., -2.],\n",
       "       [-6., -4., -2.,  0.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -2.        , -4.        , -5.99999984],\n",
       "       [-2.        , -4.        , -5.99998012, -4.        ],\n",
       "       [-4.        , -5.9999767 , -4.        , -2.        ],\n",
       "       [-5.99999997, -4.        , -2.        ,  0.        ]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = td_learning(1000, 0.5, 0.5)\n",
    "\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -2.        , -4.        , -5.99949074],\n",
       "       [-2.        , -3.99999637, -5.98801623, -3.99999997],\n",
       "       [-4.        , -5.99817449, -3.99999629, -2.        ],\n",
       "       [-5.99989131, -4.        , -2.        ,  0.        ]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = td_learning(500, 0.5, 0.5)\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -2.        , -4.        , -5.99999076],\n",
       "       [-2.        , -3.99999989, -5.99952092, -4.        ],\n",
       "       [-4.        , -5.99918569, -3.99999994, -2.        ],\n",
       "       [-5.99998697, -4.        , -2.        ,  0.        ]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = td_learning(650, 0.5, 0.5)\n",
    "\n",
    "grid = np.zeros((4,4))\n",
    "\n",
    "for state in Q.keys(): \n",
    "    \n",
    "    grid[state[0], state[1]] = max(Q[state])\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def create_maze_path(shape): \n",
    "    \n",
    "    path_coordinates = []\n",
    "    possible_coords = []\n",
    "    coord_count = 0\n",
    "    #generate a length for an array: \n",
    "    maze_length = random.randint(9,11);\n",
    "   \n",
    "    \n",
    "    # generate a starting coordinate (always in the first row)\n",
    "    starting_col = random.randint(0, shape[1] - 1); # has to be shape[1] because the length of a row is equal to the number of columns there are.\n",
    "    path_coordinates.append((0, starting_col)) # append to maze path list\n",
    "    coord_count += 1\n",
    "    \n",
    "    while coord_count < maze_length: \n",
    "    \n",
    "        possible_coords = []\n",
    "        \n",
    "        current_pos = path_coordinates[-1] # holds the most recent path position\n",
    "        if (current_pos[1] + 1 < shape[1]):  # if the previous column + 1 coordinate is less than the column shape, that means it can move to the right  \n",
    "                #print(1)\n",
    "                possible_coords.append((current_pos[0], current_pos[1] + 1))\n",
    "                \n",
    "            \n",
    "        if (current_pos[1] - 1 >=  0): # if the previous column coordinate - 1 is less greater than or equal to 0, that means it can move to the left  \n",
    "                \n",
    "          \n",
    "            possible_coords.append((current_pos[0], current_pos[1] - 1))\n",
    "            \n",
    "            \n",
    "        if (current_pos[0] + 1 < shape[0]): # if the previous row coordinate + 1 is less than the row shape, that means it can move down\n",
    "               \n",
    "         \n",
    "            possible_coords.append((current_pos[0] + 1, current_pos[1]))\n",
    "            \n",
    "            \n",
    "        if (current_pos[0] - 1 >= 0): # if the previous row coordinate - 1 is greater than or equal to 0, that means it can move up. \n",
    "        \n",
    "            possible_coords.append((current_pos[0] - 1,current_pos[1]))\n",
    "        \n",
    "        \n",
    "        # get rid of any repeats in path_coordinates\n",
    "        possible_coords = list(set(possible_coords))\n",
    "        \n",
    "        random_next_coordinate = random.randint(0, len(possible_coords) - 1)\n",
    "        \n",
    "        # so now that we don't want repeats, we need a way that we can generate another random indice. \n",
    "        if possible_coords[random_next_coordinate] not in path_coordinates: \n",
    "\n",
    "            path_coordinates.append(possible_coords[random_next_coordinate]) # add the new coordinate to the path.\n",
    "        else: \n",
    "            coord_count -= 1\n",
    "           \n",
    "        coord_count += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    return path_coordinates, shape\n",
    "    \n",
    "def generate_maze(coords, shape): \n",
    "    \n",
    "    #print(coords)\n",
    "    maze = np.random.randint(1,2, size=(shape))\n",
    "    all_coords = coords # extra_pos needs to come first so the ending position (goal remains clear)\n",
    "        \n",
    "    for coord in all_coords: \n",
    "        maze[coord[0], coord[1]] = 0\n",
    "        \n",
    "    goal = all_coords[-1]\n",
    "            \n",
    "    return maze, goal\n",
    "    \n",
    "\n",
    "def add_pos(coords, shape): \n",
    "    \n",
    "    # you can only add an extra zero at some place IF it is available. \n",
    "    # how many dead ends can you add? that number can be generated, based on the size of the maze. note that NOT all have to be used up. I will only set a max number. \n",
    "    # first, generate an amount of extra positions you can have\n",
    "    \n",
    "    num_extra = random.randint(2,4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    candidates = [] # holds all possible extra positions coordinates (with repeats)\n",
    "\n",
    "    \n",
    "    coord_count = 1 # because if it's 0, we can't do row - 1\n",
    "    \n",
    "    while coord_count <= (len(coords) - 2): \n",
    "        \n",
    "        \n",
    "        rand_pos = random.randint(0,len(coords) - 1)\n",
    "        all_taken = True\n",
    "        \n",
    "        row = coords[rand_pos][0]\n",
    "        \n",
    "        col = coords[rand_pos][1]\n",
    "       \n",
    "        \n",
    "        # need to check the bounds of the 2D maze\n",
    "        if ( (row, col + 1) not in coords and (col + 1) < shape[1] ):\n",
    "            candidates.append((row, col +1))\n",
    "            \n",
    "        if ( (row, col -1) not in coords and (col - 1) > 0):\n",
    "            candidates.append((row, col-1))\n",
    "            \n",
    "        if ( (row + 1, col) not in coords and (row + 1) < shape[0]):\n",
    "            candidates.append((row+1, col))\n",
    "            \n",
    "        if ( (row-1, col) not in coords and (row -1) > 0):\n",
    "            candidates.append((row-1, col))\n",
    "            \n",
    "        coord_count += 1\n",
    "    \n",
    "    final_candidates = list(set(candidates)) # set of coordinates without repeats. \n",
    "    \n",
    "    #print(final_candidates)\n",
    "    #print(len(final_candidates))    \n",
    "    final_num_extra = random.randint(1, len(final_candidates)) # get the number of extra dead ends you can have. \n",
    "    #print(final_num_extra)\n",
    "    extra_pos = random.sample(final_candidates, final_num_extra)\n",
    "    \n",
    "    return extra_pos\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 0, 0, 1, 1],\n",
      "       [1, 0, 0, 1, 1],\n",
      "       [1, 1, 0, 0, 1],\n",
      "       [1, 1, 1, 0, 1],\n",
      "       [0, 0, 0, 0, 1]]), (4, 0))\n"
     ]
    }
   ],
   "source": [
    "coords, shape = create_maze_path((5,5))\n",
    "#extra_pos = add_pos(coords, shape)\n",
    "print(generate_maze(coords, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
