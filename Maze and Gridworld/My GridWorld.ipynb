{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.zeros((4,4)) # represents a gridworld in which each state, apart from the terminal state, has a value of 0. \n",
    "terminal_state = [(0,0), (3,3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Check\n",
    "\n",
    "What is value? The sum of all future rewards that can be recieved from a given state s. These values are \"long lasting. Tells us the long term effects of being in a certain state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation for Gridworld \n",
    "\n",
    "What is iterative policy evaluation? Finding the true value function for each state with full knowledge of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the value of each state given the policy\n",
    "def policy_eval(maze_values): \n",
    "  \n",
    "    # WHY CAN'T I DO IT. \n",
    "                            \n",
    "    #action = random.randint(0, (1/policy) - 1)\n",
    "            # 0 = up, 1 = down, 2 = right, 3 = left. maybe put this in a dictionary or something\n",
    "    # make a for loop for the states:\n",
    "    \n",
    "    new_values = maze_values.copy()\n",
    "    \n",
    "    for row in range(len(maze_values[0])): \n",
    "        for col in range(len(maze_values[1])):\n",
    "            reward = -1 # -1 for up, down, left, right\n",
    "            successor_val = 0\n",
    "            initial_state_val = maze_values[row][col]\n",
    "            \n",
    "            if ((row, col) in terminal_state): \n",
    "                new_values[row][col] = 0\n",
    "            else:\n",
    "                # check left\n",
    "                if (col - 1 < 0): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row][col - 1])\n",
    "                # check right\n",
    "                if (col + 1 >= len(maze_values[1])): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row][col + 1])\n",
    "                # check up\n",
    "                if (row - 1 < 0): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row - 1][col])\n",
    "                # check down\n",
    "                if (row + 1 >= len(maze_values[0])): \n",
    "                    successor_val += reward + maze_values[row][col]\n",
    "                else: \n",
    "                    successor_val += (reward + maze_values[row+1][col])\n",
    "                \n",
    "                new_values[row][col] = 0.25 * successor_val\n",
    "                \n",
    "            \n",
    "        \n",
    "    return np.array(new_values) \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "maze_values = np.zeros((4,4))\n",
    "\n",
    "print(np.array(maze_values))\n",
    "\n",
    "current_values = maze_values\n",
    "\n",
    "for i in range(1000): \n",
    "    \n",
    "    current_values = policy_eval(current_values)\n",
    "    \n",
    "    \n",
    "print(current_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction\n",
    "\n",
    "How is MC Prediction different than Iterative Policy Evaluation? We do NOT have full knowledge of the environment. We find the true value function through episodes\n",
    "\n",
    "What is First Visit Monte Carlo? Averaging returns for a state after the state's first occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(pos): \n",
    "    \n",
    "    action = random.randint(0,3) # random policy\n",
    "    successor_val = 0\n",
    "    successor_state = pos\n",
    "    reward = -1\n",
    "    \n",
    "    if action == 0: # check up\n",
    "        if not (pos[0] - 1 < 0): \n",
    "            successor_state = (pos[0] - 1, pos[1])\n",
    "    if action == 1: # check down\n",
    "        if not (pos[0] + 1 > 3): \n",
    "            successor_state = (pos[0] + 1, pos[1])\n",
    "    if action == 2: # check left\n",
    "        if not (pos[1] - 1 < 0): \n",
    "            successor_state = (pos[0], pos[1] - 1)\n",
    "    if action == 3: # check right\n",
    "        if not (pos[1] + 1 > 3): \n",
    "            successor_state = (pos[0], pos[1] + 1)\n",
    "    \n",
    "    if successor_state in terminal_state: \n",
    "        reward = 0\n",
    "        \n",
    "    if pos in terminal_state:\n",
    "        reward = 0\n",
    "                               \n",
    "    \n",
    "    return pos, action, reward, successor_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(num_iterations): \n",
    "    \n",
    "    #maze_values = np.zeros((4,4))\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    state_rewards = defaultdict(int)\n",
    "    returns = defaultdict(int)\n",
    "    all_states = np.array([])\n",
    "    \n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        \n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "        states_visited = np.array([])\n",
    "        while not is_terminal:\n",
    "            initial_state, action, reward, successor_state = step(initial_state)\n",
    "            states_visited = np.append(states_visited, [initial_state, action, reward, successor_state])\n",
    "            initial_state = successor_state\n",
    "            if initial_state in terminal_state: \n",
    "                states_visited = np.copy(states_visited.reshape(int(len(states_visited)/4), 4))\n",
    "                all_states = states_visited\n",
    "                is_terminal = True\n",
    "        \n",
    "    \n",
    "        for i in range(len(all_states)): \n",
    "      \n",
    "            if all_states[i][0] not in state_rewards.keys(): \n",
    "                state_rewards[all_states[i][0]] = np.sum(all_states[i:, 2])\n",
    "       \n",
    "        \n",
    "   \n",
    "    for state, reward in state_rewards.items(): \n",
    "            \n",
    "        returns[state] = reward / num_iterations\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    return returns, state_rewards\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "            \n",
    "            \n",
    "    # there is still something wrong with this        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int,\n",
       "             {(0, 0): -0.007,\n",
       "              (0, 1): -0.007,\n",
       "              (1, 1): -0.006,\n",
       "              (0, 2): -0.004,\n",
       "              (0, 3): -0.003,\n",
       "              (3, 0): -0.008,\n",
       "              (2, 0): -0.006,\n",
       "              (1, 0): -0.001,\n",
       "              (3, 1): -0.002,\n",
       "              (3, 2): 0.0,\n",
       "              (2, 1): -0.007,\n",
       "              (2, 3): -0.026,\n",
       "              (2, 2): -0.024,\n",
       "              (1, 2): -0.021,\n",
       "              (1, 3): -0.016,\n",
       "              (3, 3): 0.0}),\n",
       " defaultdict(int,\n",
       "             {(0, 0): -7,\n",
       "              (0, 1): -7,\n",
       "              (1, 1): -6,\n",
       "              (0, 2): -4,\n",
       "              (0, 3): -3,\n",
       "              (3, 0): -8,\n",
       "              (2, 0): -6,\n",
       "              (1, 0): -1,\n",
       "              (3, 1): -2,\n",
       "              (3, 2): 0,\n",
       "              (2, 1): -7,\n",
       "              (2, 3): -26,\n",
       "              (2, 2): -24,\n",
       "              (1, 2): -21,\n",
       "              (1, 3): -16,\n",
       "              (3, 3): 0}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns = first_visit_mc(1000)\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3fb7a385c2c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaze_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaze_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mmaze_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaze_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# construct value grid\n",
    "\n",
    "maze_values = np.zeros((4,4))\n",
    "\n",
    "for i in range(maze_values.shape[0]):\n",
    "    for j in range(maze_values.shape[1]):\n",
    "        maze_values[i][j] = returns[(i,j)]\n",
    "print(maze_values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold up, why aren't the terminal states 0. hhhh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(num_iterations, discount): \n",
    "    \n",
    "    \n",
    "    #maze_values = np.zeros((4,4))\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    \n",
    "    sa_rewards = defaultdict(int)\n",
    "    returns = defaultdict(int)\n",
    "    all_states = np.array([])\n",
    "    sa_visited = defaultdict(int)\n",
    "    \n",
    "    for it in range(1, num_iterations + 1): \n",
    "        \n",
    "        is_terminal = False\n",
    "        \n",
    "        initial_state = (random.randint(0,3), random.randint(0,3))\n",
    "        #sa_visited = np.array([])\n",
    "        while not is_terminal:\n",
    "            initial_state, action, reward, successor_state = step(initial_state)\n",
    "            sa = (initial_state, action)\n",
    "            sa_visited[sa] = reward\n",
    "            initial_state = successor_state\n",
    "            if initial_state in terminal_state: \n",
    "                is_terminal = True\n",
    "        \n",
    "    \n",
    "        sa_pairs = list(sa_visited.keys())\n",
    "        rewards = list(sa_visited.values())\n",
    "        \n",
    "        for i in range(len(sa_pairs)):\n",
    "            \n",
    "            sa_rewards[sa_pairs[i]] = sum(rewards[i:]) / num_iterations\n",
    "            \n",
    "            \n",
    "        \n",
    "        # why would I need another loop\n",
    "        #for sa, reward in sa_rewards.items(): \n",
    "            \n",
    "             #sa_rewards[sa] = reward / num_iterations\n",
    "        \n",
    "        \n",
    "   \n",
    "    print(sa_visited)\n",
    "    print(sa_rewards)\n",
    "            \n",
    "        \n",
    "    #display values in an array\n",
    "\n",
    "    values = np.zeros((4,4))\n",
    "\n",
    "        \n",
    "         \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {((0, 1), 3): -1, ((0, 2), 0): -1, ((0, 2), 1): -1, ((1, 2), 2): -1, ((1, 1), 0): -1, ((0, 1), 2): 0, ((1, 0), 2): -1, ((1, 0), 0): 0})\n",
      "defaultdict(<class 'int'>, {((0, 1), 3): -3.0, ((0, 2), 0): -2.5, ((0, 2), 1): -2.0, ((1, 2), 2): -1.5, ((1, 1), 0): -1.0, ((0, 1), 2): -0.5, ((1, 0), 2): -0.5, ((1, 0), 0): 0.0})\n"
     ]
    }
   ],
   "source": [
    "mc_control(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last should still have a -1 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
